import json
from pathlib import Path
from collections import Counter

# Paths
RAW_METADATA = Path('data/raw_metadata.json')
CLEANED_METADATA = Path('data/cleaned_metadata.json')
REPORT_FILE = Path('data/cleaning_report.txt')


def load_raw_data():
    """Load raw metadata from Day 1"""
    print("ðŸ“‚ Loading raw metadata...\n")
    
    with open(RAW_METADATA, 'r') as f:
        data = json.load(f)
    
    print(f"âœ“ Loaded {len(data)} records\n")
    return data


def analyze_data(data):
    """Analyze the raw data to understand what we're working with"""
    print("ðŸ” Analyzing raw data...\n")
    
    total = len(data)
    
    # Basic counts
    with_location = sum(1 for d in data if d.get('location_name'))
    with_country = sum(1 for d in data if d.get('country'))
    with_description = sum(1 for d in data if d.get('description'))
    downloaded = sum(1 for d in data if d.get('downloaded') == 1)
    
    # Dimension analysis
    widths = [d['width'] for d in data if 'width' in d]
    heights = [d['height'] for d in data if 'height' in d]
    aspect_ratios = [w/h for w, h in zip(widths, heights) if h > 0]
    
    landscape_count = sum(1 for w, h in zip(widths, heights) if w > h)
    portrait_count = sum(1 for w, h in zip(widths, heights) if w < h)
    square_count = sum(1 for w, h in zip(widths, heights) if w == h)
    
    # Resolution analysis
    resolutions = [w * h for w, h in zip(widths, heights)]
    avg_resolution = sum(resolutions) / len(resolutions) if resolutions else 0
    
    print("ðŸ“Š Data Quality Report:")
    print(f"  â€¢ Total records: {total}")
    print(f"  â€¢ Successfully downloaded: {downloaded}")
    print(f"  â€¢ With location name: {with_location} ({with_location/total*100:.1f}%)")
    print(f"  â€¢ With country: {with_country} ({with_country/total*100:.1f}%)")
    print(f"  â€¢ With description: {with_description} ({with_description/total*100:.1f}%)")
    
    print(f"\nðŸ“ Dimensions:")
    print(f"  â€¢ Landscape orientation (w>h): {landscape_count} ({landscape_count/total*100:.1f}%)")
    print(f"  â€¢ Portrait orientation (w<h): {portrait_count} ({portrait_count/total*100:.1f}%)")
    print(f"  â€¢ Square (w=h): {square_count}")
    
    print(f"\nðŸ–¼ï¸  Resolution:")
    print(f"  â€¢ Average resolution: {avg_resolution/1_000_000:.1f} megapixels")
    print(f"  â€¢ Average width: {sum(widths)/len(widths):.0f}px")
    print(f"  â€¢ Average height: {sum(heights)/len(heights):.0f}px")
    
    # Find duplicates
    ids = [d['id'] for d in data]
    duplicate_ids = [id for id, count in Counter(ids).items() if count > 1]
    
    print(f"\nðŸ”„ Duplicates:")
    print(f"  â€¢ Duplicate IDs found: {len(duplicate_ids)}")
    
    # Query distribution
    queries = [d['query'] for d in data if 'query' in d]
    query_counts = Counter(queries)
    
    print(f"\nðŸ”Ž Query Distribution:")
    print(f"  â€¢ Unique queries used: {len(query_counts)}")
    print(f"  â€¢ Top 5 queries:")
    for query, count in query_counts.most_common(5):
        print(f"     â€¢ '{query}': {count} images")
    
    # Location distribution
    countries = [d['country'] for d in data if d.get('country')]
    country_counts = Counter(countries)
    
    if country_counts:
        print(f"\nðŸŒ Top Countries:")
        for country, count in country_counts.most_common(5):
            print(f"     â€¢ {country}: {count} images")
    
    return {
        'total': total,
        'downloaded': downloaded,
        'duplicates': len(duplicate_ids),
        'duplicate_ids': duplicate_ids,
        'landscape_count': landscape_count,
        'portrait_count': portrait_count
    }


def remove_duplicates(data):
    """Remove duplicate images based on ID"""
    print("\nðŸ”„ Removing duplicates...\n")
    
    seen_ids = set()
    unique_data = []
    duplicates_removed = 0
    
    for item in data:
        if item['id'] not in seen_ids:
            seen_ids.add(item['id'])
            unique_data.append(item)
        else:
            duplicates_removed += 1
    
    print(f"âœ“ Removed {duplicates_removed} duplicate(s)")
    print(f"âœ“ {len(unique_data)} unique records remaining\n")
    
    return unique_data


def validate_images(data):
    """
    Apply validation rules:
    1. Must have been downloaded successfully
    2. Must be landscape orientation (width > height)
    3. Must have minimum aspect ratio (not too square - min 1.3:1)
    4. Must have minimum resolution (width >= 1920)
    5. Must have required fields
    """
    print("âœ… Validating images...\n")
    
    valid_data = []
    
    failed_download = 0
    failed_orientation = 0
    failed_aspect_ratio = 0
    failed_resolution = 0
    failed_missing_fields = 0
    
    for item in data:
        # Rule 1: Must be downloaded
        if item.get('downloaded') != 1:
            failed_download += 1
            continue
        
        # Rule 2: Must be landscape orientation
        width = item.get('width', 0)
        height = item.get('height', 0)
        
        if width <= height:
            failed_orientation += 1
            continue
        
        # Rule 3: Minimum aspect ratio (1.3:1 to avoid near-square images)
        # This filters out images like 2000x1800 (1.11:1) that look stretched
        aspect_ratio = width / height if height > 0 else 0
        if aspect_ratio < 1.3:
            failed_aspect_ratio += 1
            continue
        
        # Rule 4: Minimum resolution (1920px wide for quality)
        if width < 1920:
            failed_resolution += 1
            continue
        
        # Rule 5: Must have required fields
        required_fields = ['id', 'image_url', 'photographer_name', 'width', 'height']
        if not all(item.get(field) for field in required_fields):
            failed_missing_fields += 1
            continue
        
        # Passed all validations
        valid_data.append(item)
    
    print(f"  âœ— Failed download: {failed_download}")
    print(f"  âœ— Wrong orientation (portrait): {failed_orientation}")
    print(f"  âœ— Too close to square (ratio < 1.3): {failed_aspect_ratio}")
    print(f"  âœ— Low resolution (< 1920px): {failed_resolution}")
    print(f"  âœ— Missing required fields: {failed_missing_fields}")
    print(f"  âœ“ Passed validation: {len(valid_data)}\n")
    
    return valid_data, {
        'failed_download': failed_download,
        'failed_orientation': failed_orientation,
        'failed_aspect_ratio': failed_aspect_ratio,
        'failed_resolution': failed_resolution,
        'failed_missing_fields': failed_missing_fields
    }


def enhance_metadata(data):
    """
    Add computed fields and clean up data
    """
    print("âœ¨ Enhancing metadata...\n")
    
    for item in data:
        # Add aspect ratio
        if item.get('width') and item.get('height'):
            item['aspect_ratio'] = round(item['width'] / item['height'], 2)
        
        # Add megapixels
        if item.get('width') and item.get('height'):
            item['megapixels'] = round((item['width'] * item['height']) / 1_000_000, 1)
        
        # Clean up description (remove extra whitespace)
        if item.get('description'):
            item['description'] = ' '.join(item['description'].split())
    
    print(f"âœ“ Enhanced {len(data)} records\n")
    return data


def save_cleaned_data(data):
    """Save cleaned metadata to JSON"""
    print("ðŸ’¾ Saving cleaned data...\n")
    
    with open(CLEANED_METADATA, 'w') as f:
        json.dump(data, f, indent=2)
    
    print(f"âœ“ Saved to {CLEANED_METADATA}\n")


def generate_report(stats, cleaned_count):
    """Generate a detailed cleaning report"""
    print("ðŸ“„ Generating cleaning report...\n")
    
    report = f"""
DATA CLEANING REPORT
{'='*60}

INITIAL DATA
  â€¢ Total records: {stats['total']}
  â€¢ Successfully downloaded: {stats['downloaded']}

ISSUES FOUND
  â€¢ Duplicate images: {stats['duplicates']}
  â€¢ Portrait orientation: {stats['portrait_count']}
  â€¢ Low resolution images: (calculated during validation)

CLEANING ACTIONS
  â€¢ Duplicates removed: {stats['duplicates']}
  â€¢ Invalid images filtered: {stats['total'] - cleaned_count}

FINAL CLEAN DATASET
  â€¢ Valid images: {cleaned_count}
  â€¢ Data quality: {cleaned_count/stats['total']*100:.1f}%

{'='*60}
Generated on Day 2 - Data Cleaning
"""
    
    with open(REPORT_FILE, 'w') as f:
        f.write(report)
    
    print(f"âœ“ Report saved to {REPORT_FILE}\n")


def main():
    """Main cleaning pipeline"""
    print("ðŸ§¹ DAY 2: Data Cleaning & Validation\n")
    print("="*60 + "\n")
    
    # Step 1: Load raw data
    raw_data = load_raw_data()
    
    # Step 2: Analyze raw data
    stats = analyze_data(raw_data)
    
    # Step 3: Remove duplicates
    unique_data = remove_duplicates(raw_data)
    
    # Step 4: Validate images
    valid_data, validation_stats = validate_images(unique_data)
    
    # Step 5: Enhance metadata
    cleaned_data = enhance_metadata(valid_data)
    
    # Step 6: Save cleaned data
    save_cleaned_data(cleaned_data)
    
    # Step 7: Generate report
    generate_report(stats, len(cleaned_data))
    
    # Final summary
    print("="*60)
    print("\nðŸŽ‰ DAY 2 COMPLETE!\n")
    print(f"Summary:")
    print(f"  â€¢ Started with: {len(raw_data)} images")
    print(f"  â€¢ Removed duplicates: {stats['duplicates']}")
    print(f"  â€¢ Filtered invalid: {len(unique_data) - len(cleaned_data)}")
    print(f"  â€¢ Final dataset: {len(cleaned_data)} images")
    print(f"  â€¢ Data quality: {len(cleaned_data)/len(raw_data)*100:.1f}%")
    
    # Location coverage in final dataset
    with_location = sum(1 for d in cleaned_data if d.get('location_name') or d.get('country'))
    print(f"\nðŸ“ Location Coverage (Clean Data):")
    print(f"  â€¢ Images with location data: {with_location} ({with_location/len(cleaned_data)*100:.1f}%)")
    
    print("\nâœ… Next step: Day 3 - Load data into SQLite database")
    print("   Run: python load_sqlite.py\n")


if __name__ == '__main__':
    main()